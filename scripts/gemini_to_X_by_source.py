import os
import random
import numpy as np
from docx import Document
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import faiss
import requests
from requests_oauthlib import OAuth1

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã¨Xã®èªè¨¼æƒ…å ±ã‚’å–å¾—
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
X_API_KEY = os.getenv("X_API_KEY")
X_API_SECRET = os.getenv("X_API_SECRET")
X_ACCESS_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_TOKEN_SECRET = os.getenv("X_ACCESS_TOKEN_SECRET")

# å¿…é ˆç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
if not all([GEMINI_API_KEY, X_API_KEY, X_API_SECRET, X_ACCESS_TOKEN, X_ACCESS_TOKEN_SECRET]):
    raise ValueError("å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# Geminiã®åˆæœŸè¨­å®š
def configure_gemini(api_key):
    genai.configure(api_key=api_key)
    print("Gemini APIã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

configure_gemini(GEMINI_API_KEY)

# 1. DOCX ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def read_docx(file_path):
    doc = Document(file_path)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ã¦ã®DOCXãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
def read_all_docx_in_folder(folder_path="data"):
    all_text = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".docx"):  # DOCXãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
            file_path = os.path.join(folder_path, file_name)
            print(f"ğŸ“‚ èª­ã¿è¾¼ã¿ä¸­: {file_path}")
            text = read_docx(file_path)
            all_text.append(text)
    return "\n".join(all_text)

# 2. ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
def split_text(text, max_length=300):
    sentences = text.split('\n')
    chunks, current_chunk = [], ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# 3. åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
def compute_embeddings(chunks, model):
    return np.array(model.encode(chunks))

# 4. FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
def build_faiss_index(embeddings):
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index

# 5. ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡è„ˆæƒ…å ±ã‚’å–å¾—
def get_random_context(chunks, num_chunks=2):
    return "\n".join(random.sample(chunks, min(num_chunks, len(chunks))))

# 6. Gemini API ã‚’åˆ©ç”¨ã—ã¦ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆ
def generate_tweet_with_rag(context):
    prompt = f"""ä»¥ä¸‹ã®é–¢é€£æƒ…å ±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚ã„ãã¤ã‹é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã‚¹ãŒå‡ºæ¥ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãã®ä¸­ã‹ã‚‰ä¸€ã¤é¸ã‚“ã§ã€ãã®ã‚¨ãƒƒã‚»ãƒ³ã‚¹ã‚’ä¸å¯§èªã§100å­—å‰å¾Œã®çŸ­æ–‡ã¨ã—ã¦ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
    * è‡ªåˆ†ã®çŸ¥è­˜ã‚’æ—¢çŸ¥ã«ã™ã‚‹ã“ã¨ã‚„ã€å•ã„ã‹ã‘ã®è¡¨ç¾ã‚’ã‚„ã‚ã¦ãã ã•ã„ã€‚
    * ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚„ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«ã¤ã„ã¦ã®èª¿æŸ»ã«é–¢ã™ã‚‹è¨€åŠã‚‚ã‚„ã‚ã¦ãã ã•ã„ã€‚
    é–¢é€£æƒ…å ±:
    {context}
    """
    try:
        response = genai.GenerativeModel(model_name="gemini-1.5-pro").generate_content(contents=[prompt])
        tweet = trim_to_140_chars(response)
        return tweet
    except Exception as e:
        return f"Gemini APIã‚¨ãƒ©ãƒ¼: {e}"

# ğŸ”¹ 140å­—ä»¥å†…ã«ã€Œã€‚ã€ï¼ˆå¥ç‚¹ï¼‰ã§åã‚ã‚‹é–¢æ•°
def trim_to_140_chars(text):
    if len(text) <= 140:
        return text  # ã™ã§ã«140å­—ä»¥å†…ãªã‚‰ãã®ã¾ã¾è¿”ã™

    # ã€Œã€‚ã€ã§åŒºåˆ‡ã‚‹ï¼ˆ140å­—ä»¥å†…ã®æœ€ã‚‚å¾Œã‚ã®ã€Œã€‚ã€ã‚’æ¢ã™ï¼‰
    last_period = text[:140].rfind("ã€‚")
    if last_period != -1:
        return text[:last_period + 1]  # ã€Œã€‚ã€ã‚’å«ã‚ã¦åˆ‡ã‚‹

    # ã‚‚ã—ã€Œã€‚ã€ãŒãªã‘ã‚Œã°ã€å¼·åˆ¶çš„ã«140å­—ã§åˆ‡ã‚‹
    return text[:140]

# 7. X ã«æŠ•ç¨¿
def post_to_x(text):
    auth = OAuth1(X_API_KEY, X_API_SECRET, X_ACCESS_TOKEN, X_ACCESS_TOKEN_SECRET)
    url = "https://api.twitter.com/2/tweets"
    headers = {"Content-Type": "application/json"}
    payload = {"text": text}

    response = requests.post(url, auth=auth, headers=headers, json=payload)
    if response.status_code != 201:
        raise Exception(f"Xã¸ã®æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.status_code} {response.text}")

    print(f"âœ… Xã«æŠ•ç¨¿ã—ã¾ã—ãŸ: {text}")

if __name__ == "__main__":
    folder_path = "data"  # DOCXãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
    text = read_all_docx_in_folder(folder_path)
    chunks = split_text(text)

    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = compute_embeddings(chunks, model)
    # index = build_faiss_index(embeddings)

    # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡è„ˆæƒ…å ±ã‚’å–å¾—ã—ä¿å­˜
    context = get_random_context(chunks)
    print("=== å–å¾—ã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ æ–‡è„ˆæƒ…å ± ===")
    print(context)

    # æ–‡è„ˆã‚’å…ƒã«ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆ
    tweet = generate_tweet_with_rag(context)
    print("=== ç”Ÿæˆã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ ===")
    print(tweet)

    # Xã«æŠ•ç¨¿
    post_to_x(tweet)
